import os
import pypdf
import asyncio
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import Message

from gigachat import GigaChat
from gigachat.models import Chat, Messages, MessagesRole

class SimpleRAGBot:
    def __init__(self, gigachat_token: str, telegram_token: str):
        self.gigachat_token = gigachat_token
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents = []
        self.chunks = []
        self.embeddings = None
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.load_documents_from_folder("data")
        
        # Telegram –±–æ—Ç
        self.bot = Bot(token=telegram_token)
        self.dp = Dispatcher()
        self.setup_handlers()
    
    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)"""
        ru_chars = sum(1 for c in text if '–∞' <= c.lower() <= '—è')
        en_chars = sum(1 for c in text if 'a' <= c.lower() <= 'z')
        
        if ru_chars > 0:
            return 'ru'
        elif en_chars > 0:
            return 'en'
        else:
            return 'ru'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def get_system_prompt(self, lang: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ"""
        prompts = {
            'ru': """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - —Å–∫–∞–∂–∏ "–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö".
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.""",
            
            'en': """You are an assistant that answers questions based on documents.
Answer ONLY using information from the provided documents.
If the answer is not in the documents - say "I cannot find the answer in the documents".
Answer in English."""
        }
        return prompts.get(lang, prompts['ru'])
    
    def load_documents_from_folder(self, folder_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ PDF –∏–∑ –ø–∞–ø–∫–∏"""
        if not os.path.exists(folder_path):
            print(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        for filename in os.listdir(folder_path):
            if filename.endswith('.pdf'):
                self.load_pdf(os.path.join(folder_path, filename))
        
        self.create_chunks_and_embeddings()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {len(self.chunks)} —á–∞–Ω–∫–æ–≤")
    
    def load_pdf(self, file_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω PDF —Ñ–∞–π–ª"""
        with open(file_path, 'rb') as f:
            pdf = pypdf.PdfReader(f)
            text = ""
            for page in pdf.pages:
                text += page.extract_text() + "\n"
            
            metadata = pdf.metadata or {}
            title = metadata.get('/Title', os.path.basename(file_path))
            author = metadata.get('/Author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            
            self.documents.append({
                "text": text,
                "title": title,
                "author": author
            })
    
    def create_chunks_and_embeddings(self, chunk_size: int = 500):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        for doc in self.documents:
            text = doc["text"]
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü—ã)
            paragraphs = text.split('\n\n')
            
            for para in paragraphs:
                if para.strip():
                    if len(para) > chunk_size:
                        sentences = para.split('. ')
                        current_chunk = ""
                        
                        for sentence in sentences:
                            if len(current_chunk) + len(sentence) < chunk_size:
                                current_chunk += sentence + ". "
                            else:
                                if current_chunk:
                                    self.chunks.append({
                                        "text": current_chunk.strip(),
                                        "title": doc["title"],
                                        "author": doc["author"]
                                    })
                                current_chunk = sentence + ". "
                        
                        if current_chunk:
                            self.chunks.append({
                                "text": current_chunk.strip(),
                                "title": doc["title"],
                                "author": doc["author"]
                            })
                    else:
                        self.chunks.append({
                            "text": para.strip(),
                            "title": doc["title"],
                            "author": doc["author"]
                        })
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if self.chunks:
            chunk_texts = [chunk["text"] for chunk in self.chunks]
            self.embeddings = self.embed_model.encode(chunk_texts)
    
    def find_relevant_chunks(self, query: str, top_k: int = 3):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏"""
        if not self.chunks:
            return []
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embed_model.encode([query])
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.2:
                chunk = self.chunks[idx]
                results.append({
                    "text": chunk["text"],
                    "title": chunk["title"],
                    "author": chunk["author"],
                    "score": similarities[idx]
                })
        
        return results
    
    def ask_gigachat(self, question: str, context_chunks: list, lang: str):
        """–ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å GigaChat —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        context = "\n\n".join([
            f"[–ò–∑: {chunk['title']}, –∞–≤—Ç–æ—Ä: {chunk['author']}]\n{chunk['text']}"
            for chunk in context_chunks
        ])
        
        system_prompt = self.get_system_prompt(lang)
        
        if lang == 'ru':
            user_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ö–∏–º–∏–∏ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü–†–ê–í–ò–õ–ê:
1. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –µ—Å—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - –¥–∞–π –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–æ –≤—Å–µ–º–∏ –¥–µ—Ç–∞–ª—è–º–∏
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - —Å–∫–∞–∂–∏: "–Ø –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"
3. –í—Å–µ–≥–¥–∞ —Ü–∏—Ç–∏—Ä—É–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: "–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ê–≤—Ç–æ—Ä"
4. –ù–µ —É–ø–æ–º–∏–Ω–∞–π, —á—Ç–æ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–≤–µ—Ç–∞
5. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º, –æ–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º
–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        else:
            user_prompt = f"""Question: {question}

Document context:
{context}

You are an expert in chemistry and robotics. Answer questions ONLY based on the provided documents.

RULES:
1. If the answer is in the documents - provide a complete answer with all details
2. If the information is not in the documents - say: "I cannot find the answer in the provided documents"
3. Always cite sources at the end of the answer in the format: "Source: Document Title, Author"
4. Do not mention that you are using documents in the answer text
5. Be accurate and informative, explain complex concepts in simple language
Answer in English, using only information from the context."""
        
        try:
            giga = GigaChat(
                credentials=self.gigachat_token,
                scope="GIGACHAT_API_PERS",
                model="GigaChat-2"
            )
            
            response = giga.chat(Chat(
                messages=[
                    Messages(role=MessagesRole.SYSTEM, content=system_prompt),
                    Messages(role=MessagesRole.USER, content=user_prompt)
                ],
                temperature=0.1,
                max_tokens=1500
            ))
            
            return response.choices[0].message.content
            
        except Exception as e:
            if lang == 'ru':
                return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
            else:
                return f"Error generating answer: {str(e)}"
    
    def setup_handlers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥"""
        @self.dp.message(Command("start"))
        async def start(message: Message):
            lang = self.detect_language(message.text or "")
            if lang == 'ru':
                text = f"RAG-–±–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}\n–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º."
            else:
                text = f"RAG-bot is ready!\nLoaded documents: {len(self.documents)}\nAsk a question in Russian or English."
            await message.answer(text)
        
        @self.dp.message(Command("list"))
        async def list_docs(message: Message):
            lang = self.detect_language(message.text or "")
            
            if not self.documents:
                if lang == 'ru':
                    await message.answer("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                else:
                    await message.answer("No documents loaded")
                return
            
            if lang == 'ru':
                docs_list = "\n".join([f"‚Ä¢ {doc['title']} ({doc['author']})" 
                                     for doc in self.documents])
                await message.answer(f"–î–æ–∫—É–º–µ–Ω—Ç—ã:\n{docs_list}")
            else:
                docs_list = "\n".join([f"‚Ä¢ {doc['title']} ({doc['author']})" 
                                     for doc in self.documents])
                await message.answer(f"Documents:\n{docs_list}")
        
        @self.dp.message()
        async def handle_question(message: Message):
            question = message.text.strip()
            
            if not question:
                return
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞
            lang = self.detect_language(question)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ
            if lang == 'ru':
                status = await message.answer("üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
            else:
                status = await message.answer("üîç Searching documents...")
            
            # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            relevant_chunks = self.find_relevant_chunks(question)
            
            if not relevant_chunks:
                if lang == 'ru':
                    await status.edit_text("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
                else:
                    await status.edit_text("No relevant information found in documents")
                return
            
            # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            if lang == 'ru':
                await status.edit_text("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
            else:
                await status.edit_text("Generating answer...")
            
            answer = self.ask_gigachat(question, relevant_chunks, lang)
            
            # 3. –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = set()
            for chunk in relevant_chunks:
                sources.add(f"‚Ä¢ {chunk['title']} ({chunk['author']})")
            
            sources_text = "\n".join(sources)
            
            if lang == 'ru':
                final_answer = f"{answer}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n{sources_text}"
            else:
                final_answer = f"{answer}\n\nSources:\n{sources_text}"
            
            await status.edit_text(final_answer)
    
    async def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–æ—Ç–∞"""
        print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
        await self.dp.start_polling(self.bot)

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    GIGACHAT_TOKEN = "_token_gigachat"
    TELEGRAM_TOKEN = "_token_telegram"
    
    bot = SimpleRAGBot(GIGACHAT_TOKEN, TELEGRAM_TOKEN)
    await bot.run()

if __name__ == "__main__":
    asyncio.run(main())
