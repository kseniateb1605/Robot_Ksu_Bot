import os
import pypdf
import asyncio
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.types import Message

from gigachat import GigaChat
from gigachat.models import Chat, Messages, MessagesRole

class SimpleRAGBot:
    def __init__(self, gigachat_token: str, telegram_token: str):
        self.gigachat_token = gigachat_token
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        self.embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –î–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents = []
        self.chunks = []
        self.embeddings = None
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.load_documents_from_folder("/Users/kseniatebenkova/Desktop/data")
        
        # Telegram –±–æ—Ç
        self.bot = Bot(token=telegram_token)
        self.dp = Dispatcher()
        self.setup_handlers()
    
    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)"""
        ru_chars = sum(1 for c in text if '–∞' <= c.lower() <= '—è')
        en_chars = sum(1 for c in text if 'a' <= c.lower() <= 'z')
        
        if ru_chars > 0:
            return 'ru'
        elif en_chars > 0:
            return 'en'
        else:
            return 'ru'  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def get_system_prompt(self, lang: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ"""
        prompts = {
            'ru': """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - —Å–∫–∞–∂–∏ "–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö".
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.""",
            
            'en': """You are an assistant that answers questions based on documents.
Answer ONLY using information from the provided documents.
If the answer is not in the documents - say "I cannot find the answer in the documents".
Answer in English."""
        }
        return prompts.get(lang, prompts['ru'])
    
    def load_documents_from_folder(self, folder_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ PDF –∏–∑ –ø–∞–ø–∫–∏"""
        for filename in os.listdir(folder_path):
            if filename.endswith('.pdf'):
                self.load_pdf(os.path.join(folder_path, filename))
        
        self.create_chunks_and_embeddings()
    
    def load_pdf(self, file_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω PDF —Ñ–∞–π–ª"""
        with open(file_path, 'rb') as f:
            pdf = pypdf.PdfReader(f)
            text = ""
            for page in pdf.pages:
                text += page.extract_text() + "\n"
            
            metadata = pdf.metadata or {}
            title = metadata.get('/Title', os.path.basename(file_path))
            author = metadata.get('/Author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            
            self.documents.append({
                "text": text,
                "title": title,
                "author": author
            })
    
    def create_chunks_and_embeddings(self, chunk_size: int = 500):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        for doc in self.documents:
            text = doc["text"]
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü—ã)
            paragraphs = text.split('\n\n')
            
            for para in paragraphs:
                if para.strip():
                    if len(para) > chunk_size:
                        sentences = para.split('. ')
                        current_chunk = ""
                        
                        for sentence in sentences:
                            if len(current_chunk) + len(sentence) < chunk_size:
                                current_chunk += sentence + ". "
                            else:
                                if current_chunk:
                                    self.chunks.append({
                                        "text": current_chunk.strip(),
                                        "title": doc["title"],
                                        "author": doc["author"]
                                    })
                                current_chunk = sentence + ". "
                        
                        if current_chunk:
                            self.chunks.append({
                                "text": current_chunk.strip(),
                                "title": doc["title"],
                                "author": doc["author"]
                            })
                    else:
                        self.chunks.append({
                            "text": para.strip(),
                            "title": doc["title"],
                            "author": doc["author"]
                        })
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if self.chunks:
            chunk_texts = [chunk["text"] for chunk in self.chunks]
            self.embeddings = self.embed_model.encode(chunk_texts, convert_to_numpy=True)
    
    def find_relevant_chunks(self, query: str, top_k: int = 14):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏"""
        if not self.chunks:
            return []
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embed_model.encode([query])
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.2:
                chunk = self.chunks[idx]
                results.append({
                    "text": chunk["text"],
                    "title": chunk["title"],
                    "author": chunk["author"],
                    "score": similarities[idx]
                })
        
        return results
    
    def ask_gigachat(self, question: str, context_chunks: list, lang: str):
        """–ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å GigaChat —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        context = "\n\n".join([
            f"[–ò–∑: {chunk['title']}, –∞–≤—Ç–æ—Ä: {chunk['author']}]\n{chunk['text']}"
            for chunk in context_chunks
        ])
        
        system_prompt = self.get_system_prompt(lang)
        
        if lang == 'ru':
            user_prompt = f"""Question: {question}

Document context:
{context}

You are an expert in chemistry and robotics. Answer questions ONLY based on the provided documents.

RULES:
1. If the answer can be formulated from the documents, provide a full, detailed, and structured answer.
2. Explain complex concepts in simple language, provide analysis, and connect information from different documents if possible.
3. If there is not enough information, say: "I cannot find the answer in the provided documents."
4. Do NOT invent information or add facts not present in the context.
Answer in Russian, using only information from the context."""
        else:
            user_prompt = f"""Question: {question}

Document context:
{context}

You are an expert in chemistry and robotics. Answer questions ONLY based on the provided documents.

RULES:
1. If the answer can be formulated from the documents, provide a full, detailed, and structured answer.
2. Explain complex concepts in simple language, provide analysis, and connect information from different documents if possible.
3. If there is not enough information, say: "I cannot find the answer in the provided documents."
4. Do NOT invent information or add facts not present in the context.
Answer in English, using only information from the context."""
        
        try:
            giga = GigaChat(
                credentials=self.gigachat_token,
                scope="GIGACHAT_API_PERS",
                model="GigaChat-2",
                verify_ssl_certs=False
            )
            
            response = giga.chat(Chat(
                messages=[
                    Messages(role=MessagesRole.SYSTEM, content=system_prompt),
                    Messages(role=MessagesRole.USER, content=user_prompt)
                ],
                temperature=0.3,
                max_tokens=2000
            ))
            
            return response.choices[0].message.content
            
        except Exception as e:
            if lang == 'ru':
                return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
            else:
                return f"Error generating answer: {str(e)}"
    
    def setup_handlers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥"""
        @self.dp.message(Command("start"))
        async def start(message: Message):
            text = (
                f"RAG-bot is ready!\n"
                f"Loaded documents: {len(self.documents)}\n"
                f"Ask a question in English."
            )
            await message.answer(text)
        
        @self.dp.message(Command("list"))
        async def list_docs(message: Message):
            if not self.documents:
                await message.answer("No documents loaded")
                return
            
            docs_list = "\n".join([f"‚Ä¢ {doc['title']} ({doc['author']})" 
                                     for doc in self.documents])
            await message.answer(f"Documents:\n{docs_list}")
        
        @self.dp.message()
        async def handle_question(message: Message):
            question = message.text.strip()
            if not question:
                return
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞
            lang = self.detect_language(question)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ
            if lang == 'ru':
                status = await message.answer("üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
            else:
                status = await message.answer("üîç Searching documents...")
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            relevant_chunks = self.find_relevant_chunks(question)
            
            if not relevant_chunks:
                if lang == 'ru':
                    await status.edit_text("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
                else:
                    await status.edit_text("No relevant information found in documents")
                return
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            if lang == 'ru':
                await status.edit_text("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
            else:
                await status.edit_text("Generating answer...")
            
            answer = self.ask_gigachat(question, relevant_chunks, lang)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = set()
            for chunk in relevant_chunks:
                sources.add(f"‚Ä¢ {chunk['title']} ({chunk['author']})")
            
            sources_text = "\n".join(sources)
            
            if lang == 'ru' and answer != '–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.':
                final_answer = f"{answer}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n{sources_text}"
            elif lang == 'ru' and answer == '–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.':
                final_answer = f"{answer}"
            elif lang == 'en' and answer != 'I cannot find the answer in the provided documents.':
                final_answer = f"{answer}\n\nSources:\n{sources_text}"
            elif lang == 'en' and answer == 'I cannot find the answer in the provided documents.':
                final_answer = f"{answer}"
            
            await status.edit_text(final_answer)
    
    async def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–æ—Ç–∞"""
        print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
        await self.dp.start_polling(self.bot)

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    GIGACHAT_TOKEN = ""
    TELEGRAM_TOKEN = ""
    
    bot = SimpleRAGBot(GIGACHAT_TOKEN, TELEGRAM_TOKEN)
    await bot.run()

if __name__ == "__main__":
    asyncio.run(main())
