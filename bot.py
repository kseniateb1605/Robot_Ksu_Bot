import os
import pypdf
import asyncio
import numpy as np
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity

from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.types import Message
from aiogram.enums import ParseMode
from aiogram.client.default import DefaultBotProperties

from gigachat import GigaChat
from gigachat.models import Chat, Messages, MessagesRole

from langdetect import detect, DetectorFactory
DetectorFactory.seed = 0

try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_MODEL_AVAILABLE = True
except ImportError:
    EMBEDDING_MODEL_AVAILABLE = False
    print("sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥.")

class MultiLanguageGigaChatBot:
    def __init__(self, gigachat_token: str, telegram_token: str, chunk_size: int = 500):
        self.gigachat_token = gigachat_token
        self.telegram_token = telegram_token
        self.chunk_size = chunk_size
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è RAG
        self.loaded_documents = {} 
        self.document_chunks = {} 
        self.chunk_embeddings = {} 
        self.all_chunks = []       
        self.all_embeddings = None  
        self.chunk_to_doc = []     
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if EMBEDDING_MODEL_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                EMBEDDING_MODEL_AVAILABLE = False
        
        self.preload_and_process_documents()
        
        self.bot = Bot(
            token=telegram_token, 
            default=DefaultBotProperties(parse_mode=ParseMode.HTML)
        )
        self.dp = Dispatcher()
        self.register_handlers()

    def detect_language(self, text: str) -> str:
        try:
            return detect(text)
        except:
            return 'ru'
    
    def get_language_instruction(self, lang_code: str) -> str:
        instructions = {
            'ru': "–û—Ç–≤–µ—á–∞–π—Ç–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –∞ —Ü–∏—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: \"–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ê–≤—Ç–æ—Ä\".",
            'en': "Answer fully and cite sources only at the end in the format: \"Source: Document Title, Author\"."
        }
        return instructions.get(lang_code, instructions['ru'])
    
    def get_system_prompt(self, lang_code: str) -> str:
        prompts = {
            'ru': """–í—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ö–∏–º–∏–∏. –û—Ç–≤–µ—Ç—å—Ç–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ –≤–æ–ø—Ä–æ—Å, —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
–¶–∏—Ç–∏—Ä—É–π—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ø–∏—Å–∫–æ–º –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
"–ò—Å—Ç–æ—á–Ω–∏–∫: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ê–≤—Ç–æ—Ä".
–ù–µ –≤—Å—Ç–∞–≤–ª—è–π—Ç–µ —Å—Å—ã–ª–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - —Å–∫–∞–∂–∏—Ç–µ: "–Ø –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö".
–ë—É–¥—å—Ç–µ —Ç–æ—á–Ω—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤.""",
            'en': """You are a chemistry specialist. Answer the question fully, strictly based on the provided documents.
Cite sources only at the end in the format: "Source: Document Title, Author".
Do not insert sources after each sentence.
If the answer is not in the documents, say: "I cannot find the answer in the provided documents".
Be accurate and informative."""
        }
        return prompts.get(lang_code, prompts['ru'])
    
    def split_into_chunks(self, text: str, title: str, author: str) -> List[Tuple[str, Dict]]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏"""
        chunks = []
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        current_chunk = ""
        current_chunk_paragraphs = []
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) + 2 <= self.chunk_size:
                current_chunk += paragraph + "\n\n"
                current_chunk_paragraphs.append(paragraph)
            else:
                if current_chunk:
                    metadata = {
                        'title': title,
                        'author': author,
                        'paragraph_count': len(current_chunk_paragraphs)
                    }
                    chunks.append((current_chunk.strip(), metadata))
                
                current_chunk = paragraph + "\n\n"
                current_chunk_paragraphs = [paragraph]
        
        if current_chunk:
            metadata = {
                'title': title,
                'author': author,
                'paragraph_count': len(current_chunk_paragraphs)
            }
            chunks.append((current_chunk.strip(), metadata))
        
        if not chunks and text:
            metadata = {'title': title, 'author': author, 'paragraph_count': 1}
            chunks.append((text.strip(), metadata))
        
        return chunks
    
    def create_embeddings(self, chunks: List[str]) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        if EMBEDDING_MODEL_AVAILABLE:
            embeddings = self.embedding_model.encode(chunks)
            return embeddings
        else:
            print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (BoW)")
            from sklearn.feature_extraction.text import TfidfVectorizer
            vectorizer = TfidfVectorizer(max_features=100)
            embeddings = vectorizer.fit_transform(chunks).toarray()
            return embeddings
    
    def preload_and_process_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è RAG"""
        folder_path = "/data"
        if not os.path.exists(folder_path):
            print(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
            return
        
        print("–ó–∞–≥—Ä—É–∂–∞—é –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é PDF —Ñ–∞–π–ª—ã...")
        all_chunks_list = []
        
        for filename in os.listdir(folder_path):
            if filename.lower().endswith('.pdf'):
                try:
                    # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
                    text, title, author = self.load_pdf_from_file(
                        os.path.join(folder_path, filename)
                    )
                    self.loaded_documents[filename] = (text, title, author)
                    
                    # –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
                    chunks_with_metadata = self.split_into_chunks(text, title, author)
                    self.document_chunks[filename] = chunks_with_metadata
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    chunk_texts = [chunk for chunk, _ in chunks_with_metadata]
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    if chunk_texts:
                        embeddings = self.create_embeddings(chunk_texts)
                        self.chunk_embeddings[filename] = embeddings
                        
                        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—â–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                        for i, (chunk_text, metadata) in enumerate(chunks_with_metadata):
                            self.all_chunks.append(chunk_text)
                            self.chunk_to_doc.append({
                                'filename': filename,
                                'title': metadata['title'],
                                'author': metadata['author'],
                                'chunk_index': i
                            })
                        
                        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω: {filename} | –ß–∞–Ω–∫–æ–≤: {len(chunk_texts)}")
                    else:
                        print(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –≤ —Ñ–∞–π–ª–µ: {filename}")
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â—É—é –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if self.all_chunks:
            print("–°–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤...")
            self.all_embeddings = self.create_embeddings(self.all_chunks)
            print(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.loaded_documents)} —Ñ–∞–π–ª–æ–≤, {len(self.all_chunks)} —á–∞–Ω–∫–æ–≤")
        else:
            print("–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    def load_pdf_from_file(self, file_path: str) -> tuple[str, str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ—Ä–∞"""
        with open(file_path, 'rb') as file:
            pdf_reader = pypdf.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            if not text.strip():
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF")
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = pdf_reader.metadata or {}
            title = metadata.get('/Title')
            author = metadata.get('/Author')
            
            # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ, –ø—Ä–æ–±—É–µ–º –∏–∑ –ø–µ—Ä–≤—ã—Ö 15 —Å—Ç—Ä–æ–∫ —Ç–µ–∫—Å—Ç–∞
            if not title or title.strip() == "":
                first_lines = [line.strip() for line in text.splitlines() if line.strip()][:15]
                title = max(first_lines, key=len) if first_lines else os.path.basename(file_path)
            if not author or author.strip() == "":
                first_lines = [line.strip() for line in text.splitlines() if line.strip()][:15]
                author = next((line for line in first_lines if any(k in line.lower() for k in ["–∞–≤—Ç–æ—Ä", "by", "editor", "—Ä–µ–¥–∞–∫—Ç–æ—Ä"])), "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
            
            return text, title, author
    
    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Tuple[str, Dict]]:
        """–ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        if not self.all_chunks or self.all_embeddings is None:
            return []
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        if EMBEDDING_MODEL_AVAILABLE:
            query_embedding = self.embedding_model.encode([query])
        else:
            from sklearn.feature_extraction.text import TfidfVectorizer
            vectorizer = TfidfVectorizer(max_features=100)
            all_texts = self.all_chunks + [query]
            vectorizer.fit(all_texts)
            query_embedding = vectorizer.transform([query]).toarray()
        
        similarities = cosine_similarity(query_embedding, self.all_embeddings)[0]
        
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results = []
        for idx in top_indices:
            if idx < len(self.all_chunks):
                chunk_text = self.all_chunks[idx]
                doc_info = self.chunk_to_doc[idx]
                results.append((chunk_text, doc_info))
        
        return results
    
    def get_gigachat_response(self, question: str, context_chunks: List[Tuple[str, Dict]], lang_code: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç GigaChat –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            giga = GigaChat(
                credentials=self.gigachat_token,
                scope="GIGACHAT_API_PERS",
                model="GigaChat-2",
                verify_ssl_certs=False
            )
            
            system_prompt = self.get_system_prompt(lang_code)
            language_instruction = self.get_language_instruction(lang_code)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            context_parts = []
            for chunk_text, doc_info in context_chunks:
                context_parts.append(f"Document: {doc_info['title']} by {doc_info['author']}\n{chunk_text}")
            
            context_text = "\n\n---\n\n".join(context_parts)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_snippet = context_text[:6000]
            
            user_message = f"{question}\n\nRelevant documents:\n{context_snippet}\n\n{language_instruction}"
            
            payload = Chat(
                messages=[
                    Messages(role=MessagesRole.SYSTEM, content=system_prompt),
                    Messages(role=MessagesRole.USER, content=user_message)
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            response = giga.chat(payload)
            return response.choices[0].message.content
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ GigaChat: {str(e)}"
    
    def register_handlers(self):
        self.dp.message(Command("start"))(self.cmd_start)
        self.dp.message(Command("list"))(self.cmd_list)
        self.dp.message(Command("help"))(self.cmd_help)
        self.dp.message(Command("stats"))(self.cmd_stats)
        self.dp.message(F.text)(self.handle_text_message)
    
    async def cmd_start(self, message: Message):
        files_count = len(self.loaded_documents)
        chunks_count = len(self.all_chunks)
        welcome_text = f"""
<b>Multi-Language Chemistry RAG Bot</b>

–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {files_count}
‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {chunks_count}
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RAG —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º

–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å, –∏ –±–æ—Ç –Ω–∞–π–¥–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö!
"""
        await message.answer(welcome_text)
    
    async def cmd_help(self, message: Message):
        help_text = """
<b>–ü–æ–º–æ—â—å / Help</b>

–ö–æ–º–∞–Ω–¥—ã:
/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É
/list - –ø–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª—ã
/stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
/help - —Å–ø—Ä–∞–≤–∫–∞

–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è:
‚Ä¢ RAG (Retrieval-Augmented Generation)
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º
‚Ä¢ –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ —Ö–∏–º–∏–∏ –∏–ª–∏ —Ä–æ–±–æ—Ç–∏–∑–∞—Ü–∏–∏!
"""
        await message.answer(help_text)
    
    async def cmd_list(self, message: Message):
        if not self.loaded_documents:
            await message.answer("üì≠ –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return
        
        files_info = []
        for filename, (text, title, author) in self.loaded_documents.items():
            chunks_count = len(self.document_chunks.get(filename, []))
            files_info.append(f"{filename}\n   Title: {title}\n   Author: {author}\n   Chunks: {chunks_count}")
        
        await message.answer(f"<b>–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:</b>\n\n" + "\n\n".join(files_info))
    
    async def cmd_stats(self, message: Message):
        stats_text = f"""
<b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG-—Å–∏—Å—Ç–µ–º—ã</b>

‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.loaded_documents)}
‚Ä¢ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(self.all_chunks)}
‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {self.chunk_size} —Å–∏–º–≤–æ–ª–æ–≤
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —ç–º–±–µ–¥–¥–∏–Ω–≥: {'sentence-transformers' if EMBEDDING_MODEL_AVAILABLE else '—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π TF-IDF'}

–ß–∞–Ω–∫–æ–≤ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º:"""
        
        for filename, chunks in self.document_chunks.items():
            stats_text += f"\n‚Ä¢ {filename}: {len(chunks)} —á–∞–Ω–∫–æ–≤"
        
        await message.answer(stats_text)
    
    async def handle_text_message(self, message: Message):
        question = message.text.strip()
        if not self.loaded_documents:
            await message.answer("üì≠ –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        lang_code = self.detect_language(question)
        
        # –ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –ø—Ä–æ—Ü–µ—Å—Å–µ
        processing_msg = await message.answer("<b>–ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...</b>")
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é RAG
        relevant_chunks = self.search_relevant_chunks(question, top_k=5)
        
        if not relevant_chunks:
            await processing_msg.edit_text("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.")
            return
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ GigaChat —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
        response = self.get_gigachat_response(question, relevant_chunks, lang_code)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        sources_info = "\n".join([f"‚Ä¢ {doc_info['title']} by {doc_info['author']}" 
                                  for _, doc_info in relevant_chunks])
        
        formatted_response = f"""
<b>–í–æ–ø—Ä–æ—Å:</b> {question}

<b>–û—Ç–≤–µ—Ç:</b>
{response}

<b>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</b>
{sources_info}

<i>–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG-—Å–∏—Å—Ç–µ–º—ã</i>
"""
        
        await processing_msg.edit_text(formatted_response)
    
    async def run(self):
        print(f"–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(self.loaded_documents)}, —á–∞–Ω–∫–æ–≤: {len(self.all_chunks)}")
        await self.dp.start_polling(self.bot)


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞
GIGACHAT_TOKEN = ""
TELEGRAM_TOKEN = ""

async def main():
    if not EMBEDDING_MODEL_AVAILABLE:
        print("–î–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
    
    bot = MultiLanguageGigaChatBot(GIGACHAT_TOKEN, TELEGRAM_TOKEN, chunk_size=500)
    await bot.run()

if __name__ == "__main__":
    asyncio.run(main())
